# OptimizationFinalProject
Studied a paper to learn about an optimization algorithm called Adam, which is an adaptation of stochastic gradient descent. Implemented the algorithm in Python and did some experiments to see how it performed with various parameter settings, comparing it to standard gradient descent and vanilla stochastic gradient descent. 
